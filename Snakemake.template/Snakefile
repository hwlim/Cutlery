########################
## Basic parameters

## Sample information file with five columns: id / name / group / fq1 / fq2
src_sampleInfo	= "./sample.tsv"

## Genome folder depending on platform: CCHMC:HPC vs my desktop
import socket
hostname = socket.gethostname()	
if( hostname == "EA19-00359" ):
	# my desktop
	genomeFa	= "/Users/limc8h/Research/Common_Data/hg38/genome/genome.fa"
	chrom_size	= "/Users/limc8h/Research/Common_Data/hg38/chrom.sizes"
	baseDir		= "/Volumes"
else:
	# cluster system
	genomeFa	= "/data/limlab/Resource/GenomeData/hg/hg38/Genome/genome.fa"
	chrom_size	= "/data/limlab/Resource/GenomeData/hg/hg38/Genome/chrom.size"
	baseDir		= "/data"

## Other essential pameters
genome			= "hg38"
adapter			= "AGATCGGAAGAGC"	# illumina universal adapter
#adapter			= "CTGTCTCTTATA"	# Nextera adapter for ATAC-seq or Cut&Tag
#trim_maxLen=100	## Maximum read length after trimming. ** NOT YET IMPLEMENTED **
trim_minLen		= 20
trim_minQual	= 20
chrRegex		= "chr[0-9XY]+$"

## blacklist regions for peak calling, "NULL" if N/A
peak_mask		= "/data/limlab/Resource/GenomeData/hg/hg38/ENCODE-blacklist.bed"


########################
## STAR index & options
star_index	= "/data/limlab/Resource/STAR_Index/hg38_allChr_v29_plusDm6"

## ChIP-seq / ATAC-seq / Cut&Run
star_option	= "--alignSJDBoverhangMin 999 --alignIntronMax 1 --alignMatesGapMax 2000 --outFilterMultimapNmax 1 --outFilterMismatchNoverLmax 0.05 --outReadsUnmapped None --outSAMunmapped Within"
## Note:
## Additoinal star_option to prevent soft-clipping: "--alignEndsType EndToEnd"
## BAM sort by "--outSAMtype" is handled by the star.align.sh by -s option.
## To keep unmapped reads in the output bam file, add "--outSAMunmapped Within"

#########################
## Job flags
doTrim		= False
doDedup		= True

#########################
## Directories
fastqDir	= baseDir + "/iwafuchilab/NGS_Data_NextSeq/Seq002_MG.Exp023.025_ChIP.CnR_NextSeq-082719/fastq_files"
trimDir		= "0.Fastq.Trim"
alignDir	= "1.1.Align"
filteredDir	= "1.2.Align.filtered"
dedupDir	= "1.3.Align.dedup"
splitDir	= "1.4.Align.split"
fragDir		= "1.4.Align.allFrag"
baseFreqDir = filteredDir + "/BaseFreq"
fragLenDir 	= dedupDir + "/fragLenHist"
spikeinCntDir = fragDir + "/spikeinCnt"
bigWigDir	= "2.BigWig"
bigWigDir_sub	= "2.BigWig.subInput"
#bigWigDir_spike	= "3.BigWig_spike"
#bigWigDir_spike_sub	= "3.BigWig_spike.subInput"
homerDir	= "3.Homer"

## CODE used in the initial version
#def getfq(wildcards):
#	return "0.Fastq/" + samples_indexById.loc[wildcards.sampleId, ["Fq1","Fq2"]]
#
#def getfq_trim(wildcards):
#	sampleId = samples_indexByName.loc[wildcards.sampleName, ["Id"]]
#	return [ "0.Fastq/Trim/" + sampleId + "_1.trim.fq.gz", "0.Fastq/Trim/" + sampleId + "_2.trim.fq.gz" ]

################################
## Loading sample Information
import pandas as pd
import sys
samples = pd.read_csv(src_sampleInfo, sep="\t", comment="#", na_filter=False)
if not samples.Id.is_unique:
	print( "Error: Id column in sample.tsv is not unique")
	sys.exit()
if not samples.Name.is_unique:
	print( "Error: Name column in sample.tsv is not unique")
	sys.exit()

#################################
## Cluster configuration file
#cluster = json.load(open("./cluster.json"))
import yaml
with open('cluster.yml', 'r') as fh:
	cluster = yaml.load(fh)
#    cluster = yaml.full_load(fh)





#########################
## Rules start
rule all:
	input:
		## output from check_baseFreq
#		expand(filteredDir + "/BaseFreq/{sampleName}.filtered.R{read}.freq.line.png", sampleName=samples.Name.tolist(), read=[1,2]),
		## output from make_bigwig
		expand(bigWigDir + "/{sampleName}.bw", sampleName=samples.Name.tolist()),
		expand(bigWigDir_sub + "/{sampleName}.subInput.bw", sampleName=samples.Name[samples.Ctrl != "NULL"].tolist()),
		#expand(homerDir + "/{sampleName}/HomerPeak/peak.homer.exBL.1rpm.bed", sampleName=samples.Name.tolist()),
		## fragment bed file
		#expand(fragDir + "/{sampleName}.frag.bed.gz", sampleName=samples.Name.tolist())
		## fragment length distribution
		expand(fragLenDir + "/{sampleName}.dist.png", sampleName=samples.Name.tolist()),
#		expand(spikeinCntDir + "/{sampleName}.spikeCnt.txt", sampleName=samples.Name.tolist())
		spikeinCntDir + "/spikein.txt"
		## homer tag dir
		#expand(homerDir + "/{sampleName}/TSV", sampleName=samples.Name.tolist())

		## output from trim
#		expand(trimDir + "/{sampleId}_1.trim.fq.gz", sampleId=samples["Id"].tolist()),
#		expand(trimDir + "/{sampleId}_2.trim.fq.gz", sampleId=samples["Id"].tolist())
#		expand("1.4.Align.split/{sampleId}.filtered.dedup.{group}.{proctype}.bed.gz", 
#			sampleId=samples["Id"].tolist(),
#			group=["nfr","nuc"],
#			proctype=["con","ctr","sep"])
#		expand("1.3.Align.dedup/{sampleId}.filtered.dedup.bam", sampleId=samples["Id"].tolist()),
#		expand("1.2.Align.filtered/{sampleId}.filtered.bam", sampleId=samples.index.values.tolist()),

#rule clean:
#	shell:
#		"rm -rf " + " ".join([ trimDir, alignDir, filteredDir, dedupDir, splitDir, baseFreqDir, bigWigDir ])

rule trim_pe:
	input:
		fq1 = lambda wildcards: fastqDir + "/" + samples.Fq1[samples.Id == wildcards.sampleId],
		fq2 = lambda wildcards: fastqDir + "/" + samples.Fq2[samples.Id == wildcards.sampleId]
	output:
		fq1 = trimDir + "/{sampleId}_1.trim.fq.gz",
		fq2 = trimDir + "/{sampleId}_2.trim.fq.gz"
	message:
		"Trimming... [{wildcards.sampleId}]"
	params:
		adapter = adapter,
		minLen = trim_minLen,
		minQual = trim_minQual
	log:
		trimDir + "/{sampleId}.trim.log"
	shell:
		"""
		# Note: Needs to be implemented as a quantum transaction
		cutadapt -a {params.adapter} -A {params.adapter} --minimum-length {params.minLen} -q {params.minQual} \
			-o __temp__.$$.1.fq.gz -p __temp__.$$.2.fq.gz {input.fq1} {input.fq2} 2>&1 | tee {log}
		mv __temp__.$$.1.fq.gz {output.fq1}
		mv __temp__.$$.2.fq.gz {output.fq2} 
		"""

def get_fastq(wildcards):
	if doTrim:
		return [trimDir + "/" + samples.Id[samples.Name == wildcards.sampleName].tolist()[0] + "_1.trim.fq.gz",
			trimDir + "/" + samples.Id[samples.Name == wildcards.sampleName] + "_2.trim.fq.gz"]
	else:
		return [fastqDir + "/" + samples.Fq1[samples.Name == wildcards.sampleName].tolist()[0],
			fastqDir + "/" + samples.Fq2[samples.Name == wildcards.sampleName].tolist()[0]]
	
#	raise(ValueError("Unrecognized wildcard value for 'endedness': %s" % wildcards.endedness))

rule align_pe:
	input:
		get_fastq
		#fq1 = lambda wildcards: trimDir + "/" + samples.Id[samples.Name == wildcards.sampleName] + "_1.trim.fq.gz",
		#fq2 = lambda wildcards: trimDir + "/" + samples.Id[samples.Name == wildcards.sampleName] + "_2.trim.fq.gz"
	output:
		alignDir+"/{sampleName}/align.bam"
	message:
		"Aligning... [{wildcards.sampleName}]"
	params:
		index=star_index,
		option=star_option
	log:
		alignDir + "/{sampleName}/star.log"
	threads:
		cluster["align_pe"]["cpu"]
	shell:
		"""
		module load CnR/1.0
		star.align.sh -g {params.index} \
			-o {alignDir}/{wildcards.sampleName}/align \
			-t {threads} \
			-p '{params.option}' \
			{input}
		"""
#		STAR --runMode alignReads --genomeDir {params.index} \
#			--genomeLoad NoSharedMemory \
#			--readFilesIn <( zcat {input.fq1} ) <( zcat {input.fq2} ) \
#			--runThreadN {threads} \
#			{params.option} \
#			--outFileNamePrefix __temp__.$$ 2> {log}
#		mv __temp__.$$Aligned.out.bam {output}"

rule filter_align:
	input:
		alignDir+"/{sampleName}/align.bam"
	output:
		filteredDir + "/{sampleName}.filtered.bam"
	message:
		"Filtering... [{wildcards.sampleName}]"
	shell:
		"""
		module load CnR/1.0
		cnr.filterBam.sh  -o {output} -c "{chrRegex}" {input}
		"""

rule dedup_align:
	input:
		filteredDir + "/{sampleName}.filtered.bam"
	output:
		dedupDir + "/{sampleName}.dedup.bam"
	message:
		"Deduplicating... [{wildcards.sampleName}]"
	params:
		memory = "%dG" % ( cluster["dedup_align"]["memory"]/1000 - 1 )
	shell:
		"""
		module load CnR/1.0
		cnr.dedupBam.sh -m {params.memory} -o {output} -r {input}
		"""

rule check_baseFreq:
	input:
		filteredDir + "/{sampleName}.filtered.bam"
	output:
		read1 = baseFreqDir + "/{sampleName}.filtered.R1.freq.line.png",
		read2 = baseFreqDir + "/{sampleName}.filtered.R2.freq.line.png"
	message:
		"Checking baseFrequency... [{wildcards.sampleName}]"
	shell:
		"""
		module load CnR/1.0
		bamToBed.separate.sh -o {baseFreqDir} {input}
		checkBaseFreq.plot.sh -g {genomeFa} -o {baseFreqDir} {baseFreqDir}/{wildcards.sampleName}.filtered.R1.bed.gz
		checkBaseFreq.plot.sh -g {genomeFa} -o {baseFreqDir} {baseFreqDir}/{wildcards.sampleName}.filtered.R2.bed.gz
		"""

## fragment bed file for V-plot analysis ** NOT FOR PEAK-CALLING
rule make_fragment:
	input:
		dedupDir + "/{sampleName}.dedup.bam" if doDedup else filteredDir + "/{sampleName}.filtered.bam"
	output:
		fragDir + "/{sampleName}.frag.bed.gz"
	params:
		memory = "%dG" % ( cluster["make_fragment"]["memory"]/1000 - 1 )
	message:
		"Making fragment bed files... [{wildcards.sampleName}]"
	shell:
		"""
		module load CnR/1.0
		bamToFragment.sh -o {output} -l 150 -s -m {params.memory} {input}
		"""

rule count_spikein:
	input:
		fragDir + "/{sampleName}.frag.bed.gz"
	output:
		spikeinCntDir + "/{sampleName}.spikeCnt.txt"
	message:
		"Counting spikein tags... [{wildcards.sampleName}]"
	shell:
		"""
		module load CnR/1.0
		countSpikein.sh -p dm- {input} > {output}
		"""

rule make_spikeintable:
	input:
		expand(spikeinCntDir + "/{sampleName}.spikeCnt.txt", sampleName=samples.Name.tolist())
	output:
		spikeinCntDir + "/spikein.txt"
	message:
		"Counting spikein table..."
	shell:
		"""
		module load CnR/1.0
		makeSpikeCntTable.r -o {spikeinCntDir}/spikein {input}
		"""

rule get_fragLenHist:
	input:
		dedupDir + "/{sampleName}.dedup.bam"
	output:
		fragLenDir + "/{sampleName}.dist.txt",
		fragLenDir + "/{sampleName}.dist.png"
	message:
		"Checking fragment length... [{wildcards.sampleName}]"
	shell:
		"""
		module load CnR/1.0
		ngs.fragLenHist.r -o {fragLenDir}/{wildcards.sampleName} {input}
		"""

	
rule make_bigwig:
	input:
		fragDir + "/{sampleName}.frag.bed.gz"
	output:
		bigWigDir + "/{sampleName}.bw",
	message:
		"Making bigWig files... [{wildcards.sampleName}]"
	params:
		memory = "%dG" % (  cluster["make_bigwig"]["memory"]/1000 - 1 )
	shell:
		"""
		module load CnR/1.0
		cnr.bedToBigWig.sh -g {chrom_size} -m {params.memory} -o {output} {input}
		"""

def get_bigwig_input(wildcards):
	# return ordered [ctrl , target] list.
	ctrlName = samples.Ctrl[samples.Name == wildcards.sampleName]
	ctrlName = ctrlName.tolist()[0]
	return [ bigWigDir + "/" + wildcards.sampleName + ".bw", bigWigDir + "/" + ctrlName + ".bw" ]

rule make_bigwig_subtract:
	input:
		get_bigwig_input
	output:
		bigWigDir_sub + "/{sampleName}.subInput.bw",
	message:
		"Making bigWig files... [{wildcards.sampleName}]"
	#params:
	#	memory = "%dG" % (  cluster["make_bigwig_subtract"]["memory"]/1000 - 1 )
	shell:
		"""
		module load CnR/1.0
		bigWigSubtract.sh -g {chrom_size} -m 5G -t -1000 {output} {input}
		"""


rule make_tagdir:
	input:
		fragDir + "/{sampleName}.frag.bed.gz"
	output:
		directory(homerDir + "/{sampleName}/TSV")
	params:
		name = "{sampleName}"
	message:
		"Making Homer tag directory... [{wildcards.sampleName}]"
	shell:
		"""
		module load CnR/1.0
		cnr.makeHomerDir.sh -o {output} -n {params.name} {input}
		"""



#def get_ctrl(wildcards):
#	ctrlName = samples.Ctrl[samples.Name == wildcards.sampleName]
#	if ctrlName.tolist()[0].upper() == "NULL":
#		return "NULL"
#	else:
#		return homerDir + "/" + ctrlName + "/TSV"

def get_peakcall_input(wildcards):
	# return ordered [ctrl , target] list. if no ctrl, simply [target].
	ctrlName = samples.Ctrl[samples.Name == wildcards.sampleName]
	ctrlName = ctrlName.tolist()[0]
	if ctrlName.upper() == "NULL":
		return [ homerDir + "/" + wildcards.sampleName + "/TSV" ]
	else:
		return [ homerDir + "/" + ctrlName + "/TSV", homerDir + "/" + wildcards.sampleName + "/TSV" ]

rule call_peaks:
	input:
		get_peakcall_input
		#target 	= homerDir + "/{sampleName}/TSV",
		#ctrl 	= "test"
		#ctrl 	= "get_ctrl"
	output:
		homerDir + "/{sampleName}/HomerPeak/peak.homer.exBL.1rpm.bed"
	params:
		mask = peak_mask,
		peakDir = homerDir + "/{sampleName}/HomerPeak",
		optStr = lambda wildcards, input: "-i" if len(input)>1 else ""
	message:
		"Peak calling using Homer... [{wildcards.sampleName}]"
	shell:
		"""
		module load CnR/1.0
		cnr.peakCallTF.sh -o {params.peakDir} -m {params.mask} -s \"-fragLength 100\" {params.optStr} {input}
		"""
		#cnr.peakCallTF.sh -o {params.peakDir} -m {params.mask} -i {input.ctrl} {input.target}
