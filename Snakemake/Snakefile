########################
## Basic parameters

## Sample information file with five columns: id / name / group / fq1 / fq2
src_sampleInfo	= "./sample.tsv"
cluster_yml	= os.environ["CUTLERY"] + "/Snakemake/cluster.yml"

## Genome folder depending on platform: CCHMC:HPC vs my desktop
import socket
hostname = socket.gethostname()	
if( hostname == "EA19-00359" ):
	# my desktop
	genomeFa	= "/Users/limc8h/Research/Common_Data/hg38/genome/genome.fa"
	chrom_size	= "/Users/limc8h/Research/Common_Data/hg38/chrom.sizes"
	peak_mask	= "/Users/limc8h/Research/Common_Data/hg38/ENCODE-blacklist.bed"
	baseDir		= "/Volumes"
else:
	# cluster system
	genomeFa	= "/data/limlab/Resource/GenomeData/hg/hg38/Genome/genome.fa"
	chrom_size	= "/data/limlab/Resource/GenomeData/hg/hg38/Genome/chrom.size"
	peak_mask	= "/data/limlab/Resource/GenomeData/hg/hg38/hg38-blacklist.v2.bed"
	baseDir		= "/data"

## Other essential pameters
genome			= "hg38"
adapter			= "AGATCGGAAGAGC"	# illumina universal adapter
#adapter			= "CTGTCTCTTATA"	# Nextera adapter for ATAC-seq or Cut&Tag
#trim_maxLen=100	## Maximum read length after trimming. ** NOT YET IMPLEMENTED **
trim_minLen		= 20
trim_minQual	= 20
## Regular expression to filter-out non-regular chromosomes such as random/scaffolds
chrRegexAll		= "^chr[0-9XY]+$|^dm-chr[0-9XYLR]+$"
## Regular expression for target organism to exclude spike-in genome
chrRegexTarget	= "^chr[0-9XY]+$"
spikePrefix		= "dm-"

########################
## STAR index & options
#star_index	= "/data/limlab/Resource/STAR_Index/hg38_allChr_v29_plusEcoliBL21/"
star_index	= "/data/limlab/Resource/STAR_Index/hg38_allChr_v29_plusDm6"
## STAR module for BMI HPC
## Handling of protrusion needs 2.7.4 version. But many of previous data has been aligned using 2.5
## For compatibility with previous data, 2.5 is used if star_module is not defined
star_module	= "STAR/2.7.4"

## option string
## Note:
## Additoinal star_option to prevent soft-clipping: "--alignEndsType EndToEnd"
## BAM sort by "--outSAMtype" is handled by the star.align.sh by -s option.
## To keep unmapped reads in the output bam file, add "--outSAMunmapped Within"
star_option	= "--alignSJDBoverhangMin 999 --alignIntronMax 1 --alignMatesGapMax 1000 --outFilterMultimapNmax 1 --outFilterMismatchNoverLmax 0.05 --outReadsUnmapped None --alignEndsProtrude 2 ConcordantPair"

#########################
## Job flags
doTrim		= True
doDedup		= False

#########################
## Directories
fastqDir	= baseDir + "/limlab/project/0.Fastq"
trimDir		= "0.Fastq.Trim"
alignDir	= "1.1.Align"
filteredDir	= "1.2.Align.filtered"
dedupDir	= "1.3.Align.dedup"

splitDir	= "1.4.Align.split"
fclDir		= "1.4.Align.fcl"

baseFreqDir = filteredDir + "/BaseFreq"
fragLenDir 	= splitDir + "/fragLenHist"
fragAcorDir = fclDir + "/fragAutoCor"
bigWigDir	= "2.1.BigWig.RPM"
bigWigDirAllFrag = "2.2.BigWig.allFrag.RPM"
bigWigDir_avg = "2.BigWig.avg"
bigWigDir1bp = "2.BigWig.1bp.RPM"

bigWig_RPSM = "3.1.BigWig.RPSM"
bigWigAllFrag_RPSM = "3.2.BigWig.allFrag.RPSM"

homerDir	= "4.Homer"

spikeinCntDir = splitDir + "/spikeinCount"


################################################
## Loading sample Information & Validation

## Sample table loading
import pandas as pd
import sys
samples = pd.read_csv(src_sampleInfo, sep="\t", comment="#", na_filter=False)

## Id / Name column must be unique
if not samples.Id.is_unique:
	print( "Error: Id column in sample.tsv is not unique" )
	sys.exit(1)
if not samples.Name.is_unique:
	print( "Error: Name column in sample.tsv is not unique" )
	sys.exit(1)

## Group column must not overlap with Name column
## Note: this validation may not needed if generation of average bigWig is to be done
if not len(set(samples.Group).intersection(set(samples.Name)))==0:
	print( "Error: Sample Group name must not overlap with Name" )
	sys.exit(1)

#################################
## Cluster configuration file
#cluster = json.load(open("./cluster.json"))
import yaml
with open(os.path.expanduser(cluster_yml), 'r') as fh:
	cluster = yaml.load(fh)





#########################
## Rules start
rule all:
	input:
		alignDir + "/alignStat.txt",
		## output from check_baseFreq
		expand(filteredDir + "/BaseFreq/{sampleName}.R{read}.png", sampleName=samples.Name.tolist(), read=[1,2]),
		## output from make_bigwig
		expand(bigWigDir + "/{sampleName}.{fragment}.ctr.bw", sampleName=samples.Name.tolist(), fragment=["nfr","nuc"]),
		expand(bigWigDir1bp + "/{sampleName}.{strand}.bw", sampleName=samples.Name[samples.PeakMode=="factor"].tolist(), strand=["plus","minus"]),
		expand(bigWigDirAllFrag + "/{sampleName}.allFrag.bw", sampleName=samples.Name.tolist()),
		expand(bigWigAllFrag_RPSM + "/{sampleName}.allFrag.rpsm.bw", sampleName=samples.Name.tolist()),

		expand(homerDir + "/{sampleName}/HomerPeak.factor/peak.exBL.1rpm.bed", sampleName=samples.Name[samples.PeakMode=="factor"].tolist()),
		expand(homerDir + "/{sampleName}/HomerPeak.histone/peak.exBL.bed", sampleName=samples.Name[samples.PeakMode=="histone"].tolist()),
		## fragment length distribution
		expand(fragLenDir + "/{sampleName}.dist.png", sampleName=samples.Name.tolist()),
		expand(fragAcorDir + "/{sampleName}.acor.{ext}", sampleName=samples.Name.tolist(), ext=["txt","png"]),
		spikeinCntDir + "/spikein.txt"

		#expand(spikeinCntDir + "/{sampleName}.spikeCnt.txt", sampleName=samples.Name.tolist()),
#		expand(bigWigDir_avg + "/{groupName}.{fragment}.ctr.bw", groupName=samples.Group.value_counts().index[ samples.Group.value_counts() > 1 ].tolist(), fragment=["nfr","nuc"]),
		## homer tag dir
		#expand(homerDir + "/{sampleName}/TSV.{fragment}", sampleName=samples.Name.tolist(), fragment=["nfr","nuc"])


include: os.environ["CUTLERY"] + "/Snakemake/rules.pre.smk"
include: os.environ["CUTLERY"] + "/Snakemake/rules.post.smk"

