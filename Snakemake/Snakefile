########################
## Basic parameters

## Sample information file with five columns: id / name / group / fq1 / fq2
src_sampleInfo	= "./sample.tsv"

## Genome folder depending on platform: CCHMC:HPC vs my desktop
import socket
hostname = socket.gethostname()	
if( hostname == "EA19-00359" ):
	# my desktop
	genomeFa	= "/Users/limc8h/Research/Common_Data/hg38/genome/genome.fa"
	chrom_size	= "/Users/limc8h/Research/Common_Data/hg38/chrom.sizes"
	peak_mask	= "/Users/limc8h/Research/Common_Data/hg38/ENCODE-blacklist.bed"
	baseDir		= "/Volumes"
else:
	# cluster system
	genomeFa	= "/data/limlab/Resource/GenomeData/hg/hg38/Genome/genome.fa"
	chrom_size	= "/data/limlab/Resource/GenomeData/hg/hg38/Genome/chrom.size"
	peak_mask	= "/data/limlab/Resource/GenomeData/hg/hg38/hg38-blacklist.v2.bed"
	baseDir		= "/data"

## Other essential pameters
genome			= "hg38"
adapter			= "AGATCGGAAGAGC"	# illumina universal adapter
#adapter			= "CTGTCTCTTATA"	# Nextera adapter for ATAC-seq or Cut&Tag
#trim_maxLen=100	## Maximum read length after trimming. ** NOT YET IMPLEMENTED **
trim_minLen		= 20
trim_minQual	= 20
## Regular expression to filter-out non-regular chromosomes such as random/scaffolds
chrRegexAll		= "^chr[0-9XY]+$|^dm-chr[0-9XYLR]+$"
## Regular expression for target organism to exclude spike-in genome
chrRegexTarget	= "^chr[0-9XY]+$"
spikePrefix		= "dm-"

########################
## STAR index & options
#star_index	= "/data/limlab/Resource/STAR_Index/hg38_allChr_v29_plusEcoliBL21/"
star_index	= "/data/limlab/Resource/STAR_Index/hg38_allChr_v29_plusDm6"

## ChIP-seq / ATAC-seq / Cut&Run
star_option	= "--alignSJDBoverhangMin 999 --alignIntronMax 1 --alignMatesGapMax 2000 --outFilterMultimapNmax 1 --outFilterMismatchNoverLmax 0.05 --outReadsUnmapped None"
## Note:
## Additoinal star_option to prevent soft-clipping: "--alignEndsType EndToEnd"
## BAM sort by "--outSAMtype" is handled by the star.align.sh by -s option.
## To keep unmapped reads in the output bam file, add "--outSAMunmapped Within"

#########################
## Job flags
doTrim		= True
doDedup		= False

#########################
## Directories
fastqDir	= baseDir + "/iwafuchilab/NGS_Data_NextSeq/Seq003_102419_191024_NB501763_0306_AHKM27BGXC/fastq_files"
trimDir		= "0.Fastq.Trim"
alignDir	= "1.1.Align"
filteredDir	= "1.2.Align.filtered"
dedupDir	= "1.3.Align.dedup"
splitDir	= "1.4.Align.split"
fragDir		= "1.4.Align.allFrag"
fclDir		= "1.4.Align.fcl"
baseFreqDir = filteredDir + "/BaseFreq"
fragLenDir 	= fragDir + "/fragLenHist"
fragAcorDir = fclDir + "/fragAutoCor"
bigWigDir	= "2.BigWig"
bigWigDir1bp = "2.BigWig.1bp"
bigWigDirAllFrag = "2.BigWig.allFrag"
bigWigDir_avg = "2.BigWig.avg"
bigWigScaledDir = "3.1.BigWig.scaled"
bigWigScaledDir_sub = "3.2.BigWig.scaled.subInput"
homerDir	= "4.Homer"

spikeinCntDir = fragDir + "/spikeinCount"

bigWigDir_avg = "2.BigWig.avg"

## CODE used in the initial version
#def getfq(wildcards):
#	return "0.Fastq/" + samples_indexById.loc[wildcards.sampleId, ["Fq1","Fq2"]]
#
#def getfq_trim(wildcards):
#	sampleId = samples_indexByName.loc[wildcards.sampleName, ["Id"]]
#	return [ "0.Fastq/Trim/" + sampleId + "_1.trim.fq.gz", "0.Fastq/Trim/" + sampleId + "_2.trim.fq.gz" ]

################################################
## Loading sample Information & Validation
import pandas as pd
import sys
samples = pd.read_csv(src_sampleInfo, sep="\t", comment="#", na_filter=False)

## Id / Name column must be unique
if not samples.Id.is_unique:
	print( "Error: Id column in sample.tsv is not unique" )
	sys.exit(1)
if not samples.Name.is_unique:
	print( "Error: Name column in sample.tsv is not unique" )
	sys.exit(1)

## Group column must not overlap with Name column
if not len(set(samples.Group).intersection(set(samples.Name)))==0:
	print( "Error: Sample Group name must not overlap with Name" )
	sys.exit(1)

#################################
## Cluster configuration file
#cluster = json.load(open("./cluster.json"))
import yaml
with open('cluster.yml', 'r') as fh:
	cluster = yaml.load(fh)
#    cluster = yaml.full_load(fh)





#########################
## Rules start
rule all:
	input:
		## output from check_baseFreq
		expand(filteredDir + "/BaseFreq/{sampleName}.filtered.R{read}.freq.line.png", sampleName=samples.Name.tolist(), read=[1,2]),
		## output from make_bigwig
		expand(bigWigDir + "/{sampleName}.{fragment}.ctr.bw", sampleName=samples.Name.tolist(), fragment=["nfr","nuc"]),
		expand(bigWigDir1bp + "/{sampleName}.{strand}.bw", sampleName=samples.Name[samples.PeakMode=="factor"].tolist(), strand=["plus","minus"]),
		expand(bigWigDirAllFrag + "/{sampleName}.allFrag.bw", sampleName=samples.Name.tolist()),
#		expand(bigWigDir_avg + "/{groupName}.{fragment}.ctr.bw", groupName=samples.Group.value_counts().index[ samples.Group.value_counts() > 1 ].tolist(), fragment=["nfr","nuc"]),
		expand(homerDir + "/{sampleName}/HomerPeak.factor/peak.homer.exBL.1rpm.bed", sampleName=samples.Name[samples.PeakMode=="factor"].tolist()),
		expand(homerDir + "/{sampleName}/HomerPeak.histone/peak.homer.exBL.bed", sampleName=samples.Name[samples.PeakMode=="histone"].tolist()),
		## fragment bed file
		#expand(fragDir + "/{sampleName}.frag.bed.gz", sampleName=samples.Name.tolist())
		## fragment length distribution
		expand(fragLenDir + "/{sampleName}.dist.png", sampleName=samples.Name.tolist()),
		expand(fragAcorDir + "/{sampleName}.acor.{ext}", sampleName=samples.Name.tolist(), ext=["txt","png"]),
		#expand(spikeinCntDir + "/{sampleName}.spikeCnt.txt", sampleName=samples.Name.tolist()),
		spikeinCntDir + "/spikein.txt"

		## homer tag dir
		#expand(homerDir + "/{sampleName}/TSV.{fragment}", sampleName=samples.Name.tolist(), fragment=["nfr","nuc"])


include: os.environ["MY_SCRIPT_BASE"] + "/CnR/Snakemake/rules.pre.smk"
include: os.environ["MY_SCRIPT_BASE"] + "/CnR/Snakemake/rules.post.smk"

