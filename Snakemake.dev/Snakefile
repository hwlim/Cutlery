
########################
## Sample Information
import pandas as pd
import sys
samples = pd.read_csv("./sample.tsv", sep="\t", comment="#")
if not samples.Id.is_unique:
	print( "Error: Id column in sample.tsv is not unique")
	sys.exit()
if not samples.Name.is_unique:
	print( "Error: Name column in sample.tsv is not unique")
	sys.exit()

#cluster = json.load(open("./cluster.json"))
with open('cluster.yaml', 'r') as fh:
    cluster = yaml.load(fh)


########################
## Basic parameters
import socket
hostname = socket.gethostname()	# To distinguish CCHMC:HPC vs my desktop
if( hostname == "EA19-00359" ):
	# my desktop
	genomeFa="/Users/limc8h/Research/Common_Data/mm10/genome/genome.fa"
	chrom_size="/Users/limc8h/Research/Common_Data/mm10/chrom.sizes"
else:
	# cluster system
	genomeFa="/data/limlab/Resource/GenomeData/mm/mm10/Genome/genome.fa"
	chrom_size="/data/limlab/Resource/GenomeData/mm/mm10/Genome/chrom.size"
	
genome="mm10"
adapter="AGATCGGAAGAGC"
#trim_maxLen=100
trim_minLen=20
trim_minQual=20
chrRegex="chr[0-9XY]+$"

star_index="/data/limlab/Resource/STAR_Index/mm10_allChr_vM20/"
star_option="--alignSJDBoverhangMin 999 --alignIntronMax 1 --alignMatesGapMax 2000 --outFilterMultimapNmax 1 --outFilterMismatchNoverLmax 0.05 --outReadsUnmapped None"
#star_option="--outSAMtype BAM Unsorted --alignSJDBoverhangMin 999 --alignIntronMax 1 --alignMatesGapMax 2000 --outFilterMultimapNmax 1 --outFilterMismatchNoverLmax 0.05 --outReadsUnmapped None"
## Additoinal star_option to prevent soft-clipping: "--alignEndsType EndToEnd"

#########################
## Directories
fastqDir = "0.Fastq"
trimDir = fastqDir + "/Trim"
alignDir = "1.1.Align"
filteredDir = "1.2.Align.filtered"
dedupDir = "1.3.Align.dedup"
splitDir = "1.4.Align.split"
baseFreqDir = filteredDir + "/BaseFreq"
bigWigDir = "2.BigWig"

#def getfq(wildcards):
#	return "0.Fastq/" + samples_indexById.loc[wildcards.sampleId, ["Fq1","Fq2"]]
#
#def getfq_trim(wildcards):
#	sampleId = samples_indexByName.loc[wildcards.sampleName, ["Id"]]
#	return [ "0.Fastq/Trim/" + sampleId + "_1.trim.fq.gz", "0.Fastq/Trim/" + sampleId + "_2.trim.fq.gz" ]

#########################
## Rules start
rule all:
	input:
		## output from check_baseFreq
		expand(filteredDir + "/BaseFreq/{sampleName}.filtered.R{read}.freq.line.png",
			sampleName=samples.Name.tolist(), read=[1,2]),
		## output from make_bigwig
		expand(bigWigDir + "/{sampleName}.filtered.dedup.{group}.ctr.bw",
			sampleName=samples.Name.tolist(), group=["nfr","nuc"])
		## output from trim
#		expand(trimDir + "/{sampleId}_1.trim.fq.gz", sampleId=samples["Id"].tolist()),
#		expand(trimDir + "/{sampleId}_2.trim.fq.gz", sampleId=samples["Id"].tolist())
#		expand("1.4.Align.split/{sampleId}.filtered.dedup.{group}.{proctype}.bed.gz", 
#			sampleId=samples["Id"].tolist(),
#			group=["nfr","nuc"],
#			proctype=["con","ctr","sep"])
#		expand("1.3.Align.dedup/{sampleId}.filtered.dedup.bam", sampleId=samples["Id"].tolist()),
#		expand("1.2.Align.filtered/{sampleId}.filtered.bam", sampleId=samples.index.values.tolist()),

#rule clean:
#	shell:
#		"rm -rf " + " ".join([ trimDir, alignDir, filteredDir, dedupDir, splitDir, baseFreqDir, bigWigDir ])

rule trim_pe:
	input:
		fq1 = lambda wildcards: fastqDir + "/" + samples.Fq1[samples.Id == wildcards.sampleId],
		fq2 = lambda wildcards: fastqDir + "/" + samples.Fq2[samples.Id == wildcards.sampleId]
	output:
		fq1 = trimDir + "/{sampleId}_1.trim.fq.gz",
		fq2 = trimDir + "/{sampleId}_2.trim.fq.gz"
	message:
		"Trimming... [{wildcards.sampleId}]"
	params:
		adapter = adapter,
		minLen = trim_minLen,
		minQual = trim_minQual,
	log:
		trimDir + "/{sampleId}.trim.log"
	shell:
		"""
		# Needs to be implemented as a quantum transaction
		cutadapt -a {params.adapter} -A {params.adapter} --minimum-length {params.minLen} -q {params.minQual} \
			-o __temp__.$$.1.fq.gz -p __temp__.$$.2.fq.gz {input.fq1} {input.fq2} 2>&1 | tee {log}
		mv __temp__.$$.1.fq.gz {output.fq1}
		mv __temp__.$$.2.fq.gz {output.fq2} 
		"""

rule align_pe:
	input:
		fq1 = lambda wildcards: trimDir + "/" + samples.Id[samples.Name == wildcards.sampleName] + "_1.trim.fq.gz",
		fq2 = lambda wildcards: trimDir + "/" + samples.Id[samples.Name == wildcards.sampleName] + "_2.trim.fq.gz"
	output:
		alignDir+"/{sampleName}.bam"
	message:
		"Aligning... [{wildcards.sampleName}]"
	params:
		index=star_index,
		option=star_option
	log:
		alignDir + "/{sampleName}.star.log"
	threads: cluster["align_pe"]["cpu"]
	shell:
		"""
		module load CnR/1.0
		star.align.sh -g {params.index} \
			-o {alignDir}/{wildcards.sampleName} \
			-t {threads} \
			-p '{params.option}' \
			{input.fq1} {input.fq2}
		"""
#		STAR --runMode alignReads --genomeDir {params.index} \
#			--genomeLoad NoSharedMemory \
#			--readFilesIn <( zcat {input.fq1} ) <( zcat {input.fq2} ) \
#			--runThreadN {threads} \
#			{params.option} \
#			--outFileNamePrefix __temp__.$$ 2> {log}
#		mv __temp__.$$Aligned.out.bam {output}"

rule filter_align:
	input:
		alignDir+"/{sampleName}.bam"
	output:
		filteredDir + "/{sampleName}.filtered.bam"
	message:
		"Filtering... [{wildcards.sampleName}]"
	shell:
		"""
		module load CnR/1.0
		cnr.filterBam.sh  -o {output} -c "{chrRegex}" {input}
		"""

rule dedup_align:
	input:
		filteredDir + "/{sampleName}.filtered.bam"
	output:
		dedupDir + "/{sampleName}.filtered.dedup.bam"
	message:
		"Deduplicating... [{wildcards.sampleName}]"
	shell:
		"""
		module load CnR/1.0
		cnr.dedupBam.sh -m 5G -o {output} -r {input}
		"""

rule check_baseFreq:
	input:
		filteredDir + "/{sampleName}.filtered.bam"
	output:
		read1 = baseFreqDir + "/{sampleName}.filtered.R1.freq.line.png",
		read2 = baseFreqDir + "/{sampleName}.filtered.R2.freq.line.png"
	message:
		"Checking baseFrequency... [{wildcards.sampleName}]"
	shell:
		"""
		module load CnR/1.0
		bamToBed.separate.sh -o {baseFreqDir} {input}
		checkBaseFreq.plot.sh -g {genomeFa} -o {baseFreqDir} {baseFreqDir}/{wildcards.sampleName}.filtered.R1.bed.gz
		checkBaseFreq.plot.sh -g {genomeFa} -o {baseFreqDir} {baseFreqDir}/{wildcards.sampleName}.filtered.R2.bed.gz
		"""

rule split_bam:
	input:
		dedupDir + "/{sampleName}.filtered.dedup.bam"
	output:
		expand(splitDir + "/{{sampleName}}.filtered.dedup.{group}.{proctype}.bed.gz",
			group=["nfr","nuc"], proctype=["con","ctr","sep"])
	message:
		"Splitting BAM file by fragment size... [{wildcards.sampleName}]"
	shell:
		"""
		module load CnR/1.0
		cnr.splitBamToBed.sh -o {splitDir}/{wildcards.sampleName}.filtered.dedup {input}
		"""

rule make_bigwig:
	input:
		nfr = splitDir + "/{sampleName}.filtered.dedup.nfr.ctr.bed.gz",
		nuc = splitDir + "/{sampleName}.filtered.dedup.nuc.ctr.bed.gz"
	output:
		nfr = bigWigDir + "/{sampleName}.filtered.dedup.nfr.ctr.bw",
		nuc = bigWigDir + "/{sampleName}.filtered.dedup.nuc.ctr.bw"
	message:
		"Making bigWig files... [{wildcards.sampleName}]"
	shell:
		"""
		module load CnR/1.0
		cnr.bedToBigWig.sh -g {chrom_size} -m 5G -o {output.nfr} {input.nfr}
		cnr.bedToBigWig.sh -g {chrom_size} -m 5G -o {output.nuc} {input.nuc}
		"""

#rule peak_call:
#	input:
#	output:
#	message:
#	shell:
#
